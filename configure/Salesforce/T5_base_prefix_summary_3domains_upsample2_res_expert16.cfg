[model]
name = unified.prefixtuning_res_expert_gate
use_description = True
concatenate_description = True
# Should be one of (separate, concatenate)
knowledge_usage = concatenate
freeze_plm = True
freeze_prefix = False
# phm
phm_dim = 32
factorized_phm=False
phm_rank=1
# phm_rule merge strategy, should be one of (plus, concat, mat)
strategy = mat

[dataset]
data_store_path = ./data/cache
upsample_temp = 2

[seq2seq]
constructor = seq2seq_construction.meta_tuning
patience = 400

[arg_paths]
mt_maoyanyanchu = META_TUNING/mt_maoyanyanchu.cfg
mt_maicai = META_TUNING/mt_maicai.cfg
# mt_waimai = META_TUNING/mt_waimai.cfg
mt_taxi-yonghu = META_TUNING/mt_taxi-yonghu.cfg
# mt_youxuan = META_TUNING/mt_youxuan.cfg


[evaluate]
tool = metrics.meta_tuning.evaluator

[prefix_tuning]
prefix_sequence_length = 30
mid_dim = 128
prefix_dropout = 0.0

[expert]
# expert_struct should be one of (MLP_split_to_layers_w_share, MLP_per_layer_w_share)
expert_struct = MLP_split_to_layers_w_share
moe_expert_count = 16
num_base_layers = 12
# block_w_base should be like encoder_decoder_cross
block_w_base = decoder
phm_expert = True
use_xmoe = True
phm_rule_per_layer_share = False


[special_tokens]
# less = ' <'
# less_or_equal = ' <='

unk_token = '[UNK]'
sep_token = '[SEP]'
pad_token = '[PAD]'
cls_token = '[CLS]'
mask_token = '[MASK]'

[bert]
description = t5-pegasus
location = pretrained_model/chinese_t5_pegasus_base/