[model]
name = unified.combined_prefixtuning
use_description = True
concatenate_description = True
# Should be one of (separate, concatenate)
knowledge_usage = concatenate
freeze_plm = True
# params for multi-task prefix
freeze_task_specific_prefix = False

[dataset]
data_store_path = ./data/cache
upsample_temp = 2

[seq2seq]
constructor = seq2seq_construction.meta_tuning
patience = 400
threshold = 0.1

[arg_paths]
mt_maoyanyanchu = META_TUNING/mt_maoyanyanchu.cfg
mt_maicai = META_TUNING/mt_maicai.cfg
mt_waimai = META_TUNING/mt_waimai.cfg
mt_taxi-yonghu = META_TUNING/mt_taxi-yonghu.cfg
mt_youxuan = META_TUNING/mt_youxuan.cfg

[evaluate]
tool = metrics.meta_tuning.evaluator

[prefix_tuning]
# shared new prefix sequence length for multi-domain tasks
prefix_sequence_length = 8
mid_dim = 128
prefix_dropout = 0.0

[expert]
moe_expert_count = 8
num_base_layers = 6
# block_w_base should be like encoder_decoder_cross
block_w_base = decoder
phm_expert = False
use_xmoe = True
phm_rule_per_layer_share = True
share_kv_down = True

[load_multiple_prefix_module_weights_from]
mt_maicai_8 = output/T5_base_prefix_tuning/single_domain_prelen8_initial/mt_maicai/checkpoint-10000/pytorch_model.bin
mt_maoyanyanchu_8 = output/T5_base_prefix_tuning/single_domain_prelen8_initial/mt_maoyanyanchu/checkpoint-4000/pytorch_model.bin
mt_taxi-yonghu_8 = output/T5_base_prefix_tuning/single_domain_prelen8_initial/mt_taxi-yonghu/checkpoint-14000/pytorch_model.bin
mt_waimai_8 = output/T5_base_prefix_tuning/single_domain_prelen8_initial/mt_waimai/checkpoint-16000/pytorch_model.bin
mt_youxuan_8 = output/T5_base_prefix_tuning/single_domain_prelen8_initial/mt_youxuan/checkpoint-14000/pytorch_model.bin

[special_tokens]
# less = ' <'
# less_or_equal = ' <='

unk_token = '[UNK]'
sep_token = '[SEP]'
pad_token = '[PAD]'
cls_token = '[CLS]'
mask_token = '[MASK]'

[bert]
description = t5-pegasus
location = pretrained_model/chinese_t5_pegasus_base/